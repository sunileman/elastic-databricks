{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09aa660e-cde7-4ad8-bd07-5b07e88a5388",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting dbldatagen\n  Downloading dbldatagen-0.3.5-py3-none-any.whl (86 kB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 86.3/86.3 kB 2.0 MB/s eta 0:00:00\nInstalling collected packages: dbldatagen\nSuccessfully installed dbldatagen-0.3.5\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install dbldatagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eef285a6-da29-4672-84cc-8e00ea91756e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13440882-f6e6-4a41-b706-4fa687d2734b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import LongType, IntegerType, StringType\n",
    "import dbldatagen as dg\n",
    "from pyspark.sql.types import LongType, FloatType, IntegerType, StringType, \\\n",
    "                              DoubleType, BooleanType, ShortType, \\\n",
    "                              TimestampType, DateType, DecimalType, \\\n",
    "                              ByteType, BinaryType, ArrayType, MapType, \\\n",
    "                              StructType, StructField\n",
    "\n",
    "spark = SparkSession.builder.appName(\"AzureDatabricksToBlob\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11934e44-15ce-4c87-8918-d8fada3d7680",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define your Azure Blob Storage configurations\n",
    "account_name = \"\"\n",
    "account_key = \"\"\n",
    "container_name = \"\"\n",
    "blob_folder = \"dbGenOutput\"  # Optional, you can save directly to the root of the container\n",
    "blob_name = \"jsonData1.json\"  # Name of the file to be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f7844ca-9981-48bd-8efd-50acb7977b4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set configurations for Blob Storage\n",
    "spark.conf.set(f\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "spark.conf.set(f\"spark.hadoop.fs.azure.account.key.{account_name}.blob.core.windows.net\", account_key)\n",
    "output_path = f\"wasbs://{container_name}@{account_name}.blob.core.windows.net/{blob_folder}/{blob_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b6eaac-add6-42f1-a80b-fcbd1a3d8eb5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "country_codes = ['CN', 'US', 'FR', 'CA', 'IN', 'JM', 'IE', 'PK', 'GB', 'IL', 'AU', 'SG',\n",
    "                 'ES', 'GE', 'MX', 'ET', 'SA', 'LB', 'NL']\n",
    "country_weights = [1300, 365, 67, 38, 1300, 3, 7, 212, 67, 9, 25, 6, 47, 83, 126, 109, 58, 8,\n",
    "                   17]\n",
    "\n",
    "generate_number_of_json_rows = 100000000\n",
    "device_population = 100000\n",
    "\n",
    "manufacturers = ['Delta corp', 'Xyzzy Inc.', 'Lakehouse Ltd', 'Acme Corp', 'Embanks Devices']\n",
    "\n",
    "lines = ['delta', 'xyzzy', 'lakehouse', 'gadget', 'droid']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b353ddd-140a-4b3d-92b6-4edafa757f49",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "testDataSpec = (\n",
    "    dg.DataGenerator(spark, name=\"device_data_set\", rows=generate_number_of_json_rows,\n",
    "                     partitions=8, randomSeedMethod='hash_fieldname')\n",
    "    .withIdOutput()\n",
    "    # we'll use hash of the base field to generate the ids to\n",
    "    # avoid a simple incrementing sequence\n",
    "    .withColumn(\"internal_device_id\", LongType(), minValue=0x1000000000000,\n",
    "                uniqueValues=device_population, omit=True, baseColumnType=\"hash\")\n",
    "\n",
    "    # note for format strings, we must use \"%lx\" not \"%x\" as the\n",
    "    # underlying value is a long\n",
    "    .withColumn(\"device_id\", StringType(), format=\"0x%013x\",\n",
    "                baseColumn=\"internal_device_id\")\n",
    "\n",
    "    # the device / user attributes will be the same for the same device id\n",
    "    # so lets use the internal device id as the base column for these attribute\n",
    "    .withColumn(\"country\", StringType(), values=country_codes,\n",
    "                weights=country_weights,\n",
    "                baseColumn=\"internal_device_id\")\n",
    "\n",
    "    .withColumn(\"manufacturer\", StringType(), values=manufacturers,\n",
    "                baseColumn=\"internal_device_id\", omit=True)\n",
    "    .withColumn(\"line\", StringType(), values=lines, baseColumn=\"manufacturer\",\n",
    "                baseColumnType=\"hash\", omit=True)\n",
    "    .withColumn(\"manufacturer_info\", StructType([StructField('line',StringType()),\n",
    "                                                StructField('manufacturer', StringType())]),\n",
    "                expr=\"named_struct('line', line, 'manufacturer', manufacturer)\",\n",
    "                baseColumn=['manufacturer', 'line'])\n",
    "\n",
    "\n",
    "    .withColumn(\"model_ser\", IntegerType(), minValue=1, maxValue=11,\n",
    "                baseColumn=\"device_id\",\n",
    "                baseColumnType=\"hash\", omit=True)\n",
    "\n",
    "    .withColumn(\"event_type\", StringType(),\n",
    "                values=[\"activation\", \"deactivation\", \"plan change\",\n",
    "                        \"telecoms activity\", \"internet activity\", \"device error\"],\n",
    "                random=True, omit=True)\n",
    "    .withColumn(\"event_ts\", \"timestamp\", begin=\"2020-01-01 01:00:00\",\n",
    "                end=\"2020-12-31 23:59:00\",\n",
    "                interval=\"1 minute\", random=True, omit=True)\n",
    "\n",
    "    .withColumn(\"event_info\",\n",
    "                 StructType([StructField('event_type',StringType()),\n",
    "                             StructField('event_ts', TimestampType())]),\n",
    "                expr=\"named_struct('event_type', event_type, 'event_ts', event_ts)\",\n",
    "                baseColumn=['event_type', 'event_ts'])\n",
    "    )\n",
    "\n",
    "dfTestData = testDataSpec.build()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aec160fd-1535-4bc7-a638-efc318d39c20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#This will write several json files to the output directory. Scales well and is recommended\n",
    "dfTestData.write.format(\"json\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "##however if you want a single json file, use this.\n",
    "#dfTestData.coalesce(1).write.format(\"json\").mode(\"overwrite\").save(output_path)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Databricks Data Generator",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
